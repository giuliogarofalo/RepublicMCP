# Raccomandazioni Modelli Ollama per RepublicMCP

## Problema con qwen2.5:14b

Il modello `qwen2.5:14b` √® **troppo piccolo** (270 milioni di parametri) per:
- Comprendere domande complesse
- Generare JSON strutturati correttamente
- Mappare domande naturali ‚Üí tool calls

**Risultato**: Risposte poco accurate, JSON malformati, parametri sbagliati.

## Modelli Consigliati

### üåü gemma2:2b (CONSIGLIATO)

```bash
ollama pull gemma2:2b
```

**Pro**:
- ‚úÖ 2 miliardi di parametri (7x pi√π grande di 270m)
- ‚úÖ Veloce su laptop (~2-3 secondi)
- ‚úÖ Buona comprensione italiano
- ‚úÖ Ottimo per JSON strutturati
- ‚úÖ Memoria richiesta: ~2GB RAM

**Contro**:
- ‚ö†Ô∏è A volte necessita domande chiare

**Valutazione**: 8/10 - Miglior compromesso velocit√†/qualit√†

---

### ‚ö° llama3.2:3b

```bash
ollama pull llama3.2:3b
```

**Pro**:
- ‚úÖ 3 miliardi di parametri
- ‚úÖ Ottima comprensione contesto
- ‚úÖ Meta/Facebook, molto affidabile
- ‚úÖ Buon italiano

**Contro**:
- ‚ö†Ô∏è Leggermente pi√π lento (~4-5 sec)
- ‚ö†Ô∏è RAM: ~3GB

**Valutazione**: 8.5/10 - Ottimo bilanciamento

---

### üéØ qwen2.5:3b

```bash
ollama pull qwen2.5:3b
```

**Pro**:
- ‚úÖ 3 miliardi di parametri
- ‚úÖ **ECCELLENTE per task strutturati**
- ‚úÖ Ottimo con JSON e function calling
- ‚úÖ Multilingua (CN/EN/IT)

**Contro**:
- ‚ö†Ô∏è Italiano meno naturale di gemma/llama
- ‚ö†Ô∏è RAM: ~3GB

**Valutazione**: 9/10 - Migliore per tool calling!

---

### üèÜ mistral:7b

```bash
ollama pull mistral:7b
```

**Pro**:
- ‚úÖ 7 miliardi di parametri
- ‚úÖ Qualit√† superiore
- ‚úÖ Ottima comprensione
- ‚úÖ Pochi errori

**Contro**:
- ‚ùå Lento (~8-10 sec)
- ‚ùå RAM: ~6GB
- ‚ùå Non ideale per laptop standard

**Valutazione**: 9.5/10 - Migliore accuratezza, ma pesante

---

## Confronto Prestazioni

| Modello | Parametri | Velocit√† | Qualit√† | RAM | Voto |
|---------|-----------|----------|---------|-----|------|
| qwen2.5:14b | 270M | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | 512MB | 4/10 ‚ùå |
| **gemma2:2b** | 2B | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 2GB | **8/10** ‚úÖ |
| llama3.2:3b | 3B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 3GB | 8.5/10 ‚úÖ |
| **qwen2.5:3b** | 3B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 3GB | **9/10** ‚≠ê |
| mistral:7b | 7B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 6GB | 9.5/10 üèÜ |

## Come Cambiare Modello

### Metodo 1: Variabile d'ambiente

```bash
# Una volta
export OLLAMA_MODEL=gemma2:2b
npm run cli

# Oppure inline
OLLAMA_MODEL=qwen2.5:3b npm run cli
```

### Metodo 2: File .env

Crea `.env` nella root del progetto:

```bash
OLLAMA_MODEL=qwen2:2b
```

Poi avvia normalmente:
```bash
npm run cli
```

### Metodo 3: Modifica codice

Modifica `src/cli-ollama.ts`:

```typescript
const model = process.env.OLLAMA_MODEL || "gemma2:2b"; // Cambia qui
```

## Test Comparativo

### Domanda: "Chi √® il presidente del consiglio?"

**qwen2.5:14b** (‚ùå MALE):
```json
{
  "tool": "search_deputati",
  "params": {"cognome": "Meloni", "cognome": "Demosi"}, // ERRORE: chiave duplicata!
  "reasoning": "..."
}
```

**gemma2:2b** (‚úÖ BUONO):
```json
{
  "tool": "search_deputati",
  "params": {"cognome": "Meloni"},
  "reasoning": "search for Meloni who is PM"
}
```

**qwen2.5:3b** (‚úÖ OTTIMO):
```json
{
  "tool": "search_deputati",
  "params": {"cognome": "Meloni", "nome": "Giorgia"},
  "reasoning": "Current Italian PM is Giorgia Meloni"
}
```

### Domanda: "Ultime 3 leggi sull'ecologia"

**qwen2.5:14b** (‚ùå MALE):
```json
{
  "tool": "search_deputati", // TOOL SBAGLIATO!
  "params": {"cognome": "Meloni"},
  "reasoning": "..."
}
```

**gemma2:2b** (‚úÖ BUONO):
```json
{
  "tool": "search_atti",
  "params": {"titolo": "ecologia", "limit": 3},
  "reasoning": "search acts about ecology"
}
```

**qwen2.5:3b** (‚úÖ OTTIMO):
```json
{
  "tool": "search_atti",
  "params": {"titolo": "ecologia", "tipo": "legge", "limit": 3},
  "reasoning": "search for laws (legge) about ecology"
}
```

## Raccomandazione Finale

### Per laptop standard (8-16GB RAM):
```bash
ollama pull gemma2:2b
OLLAMA_MODEL=gemma2:2b npm run cli
```

### Per PC potenti (16GB+ RAM):
```bash
ollama pull qwen2.5:3b
OLLAMA_MODEL=qwen2.5:3b npm run cli
```

### Per massima qualit√† (32GB+ RAM):
```bash
ollama pull mistral:7b
OLLAMA_MODEL=mistral:7b npm run cli
```

## Troubleshooting

### "Modello troppo lento"
‚Üí Usa `gemma2:2b`

### "Risposte poco accurate"
‚Üí Passa a `qwen2.5:3b` o `mistral:7b`

### "Out of memory"
‚Üí Torna a `gemma2:2b` o aumenta RAM

### "Modello non trovato"
```bash
ollama pull gemma2:2b
```

---

**TL;DR**: Usa `gemma2:2b` o `qwen2.5:3b`, NON `qwen2.5:14b`!
